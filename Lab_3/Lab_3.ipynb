{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MlSzJZcyYVlv"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics as m\n",
        "# import foo\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "GLOB_Y:pd.DataFrame\n",
        "GLOB_X:pd.DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_with_zeros(dim):\n",
        "    w = np.zeros((dim,1))\n",
        "    b = 0.\n",
        "    return w, b\n",
        "\n",
        "def sigmoid(z):\n",
        "    s = 1./(1.+np.exp(-z))\n",
        "    return s\n",
        "\n",
        "def propagate(w, b, X, Y):\n",
        "    m = X.shape[1]\n",
        "    #print('number of objects = ',len(X))\n",
        "    \n",
        "    # FORWARD PROPAGATION (FROM X TO COST)\n",
        "    A = sigmoid(np.dot(w.T,X)+b)                                 # compute activation\n",
        "    cost = -(1./m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A),axis=1)   # compute cost\n",
        "    \n",
        "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
        "    dw = (1./m)*np.dot(X,(A-Y).T)\n",
        "    db = (1./m)*np.sum(A-Y,axis=1)\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "            \"db\": db}\n",
        "    \n",
        "    return grads, cost\n",
        "\n",
        "def corr_val(value, coef, t):\n",
        "    corr_val_x = value/(1-coef**t)\n",
        "    return corr_val_x\n",
        "\n",
        "def predict(w, b, X):\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "    \n",
        "    # Compute vector \"A\" predicting the probabilities \n",
        "    A = sigmoid(np.dot(w.T,X)+b)\n",
        "    \n",
        "    for i in range(A.shape[1]):\n",
        "        \n",
        "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
        "        if (A[0,i]<=0.5):\n",
        "            Y_prediction[0][i]=0\n",
        "        else:\n",
        "            Y_prediction[0][i]=1\n",
        "    \n",
        "    return Y_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ADAM algorithm\n",
        "beta_1 = 0.9\n",
        "beta_2 = 0.999\n",
        "epsilon = 0.00000001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gd_optimize(X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "\n",
        "    costs = []\n",
        "\n",
        "    X_trans = np.asarray(X).T\n",
        "    Y_trans = np.asarray(Y).T\n",
        "\n",
        "    # initialize parameters with zeros \n",
        "    w, b = initialize_with_zeros(X_trans.shape[0])\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "                \n",
        "        # Cost and gradient calculation \n",
        "        grads, cost = propagate(w,b,X_trans,Y_trans)\n",
        "        \n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        # update rule\n",
        "        w -=learning_rate*dw\n",
        "        b -=learning_rate*db\n",
        "        \n",
        "        # Record the costs\n",
        "        costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 100 training iterations\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    params = {\"w\": w,\n",
        "            \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "            \"db\": db}\n",
        "    \n",
        "    return params, grads, costs\n",
        "\n",
        "def sgd_optimize(X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \n",
        "    costs = []\n",
        "\n",
        "    X_trans = np.asarray(X).T\n",
        "    Y_trans = np.asarray(Y).T\n",
        "\n",
        "    # initialize parameters with zeros \n",
        "    w, b = initialize_with_zeros(X_trans.shape[0])\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        # Prepare a random subsample of data\n",
        "        sample_len = random.randint(100,200)\n",
        "        X_sample = X.sample(sample_len)\n",
        "        Y_sample = GLOB_Y[X_sample.index]\n",
        "        X_trans = np.asarray(X_sample).T\n",
        "        Y_trans = np.asarray(Y_sample).T\n",
        "        # Cost and gradient calculation \n",
        "        grads, cost = propagate(w,b,X_trans,Y_trans)\n",
        "        \n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        # update rule\n",
        "        w -=learning_rate*dw\n",
        "        b -=learning_rate*db\n",
        "        \n",
        "        # Record the costs\n",
        "        costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 100 training iterations\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    params = {\"w\": w,\n",
        "            \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "            \"db\": db}\n",
        "    \n",
        "    return params, grads, costs\n",
        "\n",
        "def v_val_corr(value, t, grad_vec):\n",
        "    v_val = beta_2*value + (1-beta_2) * np.power( grad_vec, 2)\n",
        "    return v_val, corr_val(v_val, beta_2, t)\n",
        "\n",
        "def s_val_corr(value, t, grad_vec):\n",
        "    s_val = beta_1*value + (1-beta_1) * grad_vec\n",
        "    return s_val, corr_val(s_val, beta_1, t)\n",
        "\n",
        "def adam_optimize(X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \n",
        "    costs = []\n",
        "\n",
        "    X_trans = np.asarray(X).T\n",
        "    Y_trans = np.asarray(Y).T\n",
        "\n",
        "    # initialize parameters with zeros \n",
        "    w, b = initialize_with_zeros(X_trans.shape[0])\n",
        "    w_v_val, b_v_val = initialize_with_zeros(X_trans.shape[0])\n",
        "    w_s_val, b_s_val = initialize_with_zeros(X_trans.shape[0])\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "                \n",
        "        # Cost and gradient calculation \n",
        "        grads, cost = propagate(w,b,X_trans,Y_trans)\n",
        "        \n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        w_v_val, w_v_val_corr = v_val_corr(value=w_v_val, t=i+1, grad_vec=dw) # i+1 stands to avoid devision by zero\n",
        "        b_v_val, b_v_val_corr = v_val_corr(value=b_v_val, t=i+1, grad_vec=db)\n",
        "\n",
        "        w_s_val, w_s_val_corr = s_val_corr(value=w_s_val, t=i+1, grad_vec=dw) # i+1 stands to avoid devision by zero\n",
        "        b_s_val, b_s_val_corr = s_val_corr(value=b_s_val, t=i+1, grad_vec=db)\n",
        "\n",
        "        # update rule\n",
        "        w -= learning_rate * w_s_val_corr / (np.sqrt(w_v_val_corr) + epsilon)\n",
        "        b -= learning_rate * b_s_val_corr / (np.sqrt(b_v_val_corr) + epsilon)\n",
        "        \n",
        "        # Record the costs\n",
        "        costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 100 training iterations\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    params = {\"w\": w,\n",
        "            \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "            \"db\": db}\n",
        "    \n",
        "    return params, grads, costs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q6p8pd8pYVl1"
      },
      "outputs": [],
      "source": [
        "# predict\n",
        "\n",
        "def predict(w, b, X):\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "    \n",
        "    # Compute vector \"A\" predicting the probabilities \n",
        "    A = sigmoid(np.dot(w.T,X)+b)\n",
        "    \n",
        "    for i in range(A.shape[1]):\n",
        "        \n",
        "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
        "        if (A[0,i]<=0.5):\n",
        "            Y_prediction[0][i]=0\n",
        "        else:\n",
        "            Y_prediction[0][i]=1\n",
        "    \n",
        "    return Y_prediction\n",
        "\n",
        "# model\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, optim_fun = gd_optimize, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
        "     \n",
        "    X_train_trans = np.asarray(X_train).T\n",
        "    Y_train_trans = np.asarray(Y_train).T\n",
        "    X_test_trans = np.asarray(X_test).T\n",
        "    Y_test_trans = np.asarray(Y_test).T\n",
        "\n",
        "    # Gradient descent\n",
        "    parameters, grads, costs = optim_fun(X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "    \n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    \n",
        "    # Predict test/train set examples\n",
        "    Y_prediction_test = predict(w, b, X_test_trans)\n",
        "    Y_prediction_train = predict(w, b, X_train_trans)\n",
        "\n",
        "    # Print train/test Errors\n",
        "    traina = (100 - np.mean(np.abs(Y_prediction_train - Y_train_trans)) * 100)\n",
        "    testa = (100 - np.mean(np.abs(Y_prediction_test - Y_test_trans)) * 100)\n",
        "\n",
        "    text = f'train acc = {traina}\\ntest acc = {testa}'\n",
        "\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "\n",
        "     \n",
        "    \n",
        "    return d, text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4NWUYiqoYVl3"
      },
      "outputs": [],
      "source": [
        "# Prepare data\n",
        "df = pd.read_csv(\"bioresponse.csv\")\n",
        "GLOB_Y=df['Activity'].values\n",
        "\n",
        "# choosing numeric features\n",
        "GLOB_X = df.drop(['Activity'], axis=1, inplace=False)\n",
        "GLOB_X\n",
        "\n",
        "# Create Train and Test samples\n",
        "X_train, X_test, y_train, y_test = train_test_split(GLOB_X,GLOB_Y,test_size=0.25, random_state=68)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/1x/1zh1gh3s0gn4wj0d3z8q2x4m0000gn/T/ipykernel_12744/3759655113.py:16: RuntimeWarning: divide by zero encountered in log\n",
            "  cost = -(1./m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A),axis=1)   # compute cost\n",
            "/var/folders/1x/1zh1gh3s0gn4wj0d3z8q2x4m0000gn/T/ipykernel_12744/3759655113.py:16: RuntimeWarning: invalid value encountered in multiply\n",
            "  cost = -(1./m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A),axis=1)   # compute cost\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+-------------------------------+-------------------------------+-------------------------------+\n",
            "| Learning Rate |            GD model           |      Stochastic GD model      |           ADAM model          |\n",
            "+---------------+-------------------------------+-------------------------------+-------------------------------+\n",
            "|     0.0001    | train acc = 55.52790615001778 | train acc = 55.77675079985781 | train acc = 81.76324209029505 |\n",
            "|               | test acc = 55.223880597014926 | test acc = 55.117270788912585 |  test acc = 76.97228144989339 |\n",
            "|     0.0005    | train acc = 68.68112335584786 | train acc = 68.61002488446498 | train acc = 87.55776750799858 |\n",
            "|               |  test acc = 69.5095948827292  |  test acc = 69.29637526652452 |  test acc = 75.69296375266525 |\n",
            "|     0.001     | train acc = 73.05367934589407 |  train acc = 72.9825808745112 |  train acc = 89.4418769996445 |\n",
            "|               |  test acc = 71.74840085287846 |  test acc = 71.21535181236675 |  test acc = 75.05330490405117 |\n",
            "|     0.005     | train acc = 76.85744756487736 | train acc = 76.85744756487736 |  train acc = 93.7788837539993 |\n",
            "|               |  test acc = 76.43923240938166 |  test acc = 76.65245202558636 |  test acc = 73.02771855010661 |\n",
            "|      0.01     | train acc = 78.77710629221471 | train acc = 78.81265552790614 | train acc = 94.27657305367934 |\n",
            "|               |  test acc = 77.50533049040511 |  test acc = 77.50533049040511 |  test acc = 73.02771855010661 |\n",
            "|      0.05     | train acc = 82.18983291859226 |  train acc = 82.580874511198  | train acc = 95.59189477426236 |\n",
            "|               |  test acc = 77.9317697228145  |  test acc = 77.39872068230277 |  test acc = 71.53518123667376 |\n",
            "+---------------+-------------------------------+-------------------------------+-------------------------------+\n"
          ]
        }
      ],
      "source": [
        "lern_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]\n",
        "\n",
        "table = PrettyTable()\n",
        "gd = []\n",
        "sgd = []\n",
        "adam = []\n",
        "\n",
        "dic = {\n",
        "    'gd': [gd,gd_optimize],\n",
        "    'sgd': [sgd,sgd_optimize],\n",
        "    'adam': [adam,adam_optimize]\n",
        "}\n",
        "\n",
        "table.field_names = ['Learning Rate', 'GD model', 'Stochastic GD model', 'ADAM model']\n",
        "\n",
        "for rate in lern_rates:\n",
        "    teta = []\n",
        "    teta = [str(rate)]\n",
        "    for key in dic.keys():\n",
        "        # o = dic[key][1]\n",
        "        z, rez = model(X_train, y_train, X_test, y_test, \n",
        "                            num_iterations = 2000, learning_rate = rate, \n",
        "                            print_cost = False, optim_fun = dic[key][1])\n",
        "        dic[key][0].append(z)\n",
        "        teta.append(rez)\n",
        "    table.add_row(teta)\n",
        "\n",
        "print(table)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Logistic Regression as a Neural Network - BP alg.ipynb",
      "provenance": []
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
