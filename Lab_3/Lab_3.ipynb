{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MlSzJZcyYVlv"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics as m\n",
        "# import foo\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "GLOB_Y:pd.DataFrame\n",
        "GLOB_X:pd.DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_with_zeros(dim):\n",
        "    w = np.zeros((dim,1))\n",
        "    b = 0.\n",
        "    return w, b\n",
        "\n",
        "def sigmoid(z):\n",
        "    s = 1./(1.+np.exp(-z))\n",
        "    return s\n",
        "\n",
        "def propagate(w, b, X, Y):\n",
        "    m = X.shape[1]\n",
        "    #print('number of objects = ',len(X))\n",
        "    \n",
        "    # FORWARD PROPAGATION (FROM X TO COST)\n",
        "    A = sigmoid(np.dot(w.T,X)+b)                                 # compute activation\n",
        "    cost = -(1./m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A),axis=1)   # compute cost\n",
        "    \n",
        "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
        "    dw = (1./m)*np.dot(X,(A-Y).T)\n",
        "    db = (1./m)*np.sum(A-Y,axis=1)\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "            \"db\": db}\n",
        "    \n",
        "    return grads, cost\n",
        "\n",
        "def corr_val(value, coef, t):\n",
        "    corr_val_x = value/(1-coef**t)\n",
        "    return corr_val_x\n",
        "\n",
        "def predict(w, b, X):\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "    \n",
        "    # Compute vector \"A\" predicting the probabilities \n",
        "    A = sigmoid(np.dot(w.T,X)+b)\n",
        "    \n",
        "    for i in range(A.shape[1]):\n",
        "        \n",
        "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
        "        if (A[0,i]<=0.5):\n",
        "            Y_prediction[0][i]=0\n",
        "        else:\n",
        "            Y_prediction[0][i]=1\n",
        "    \n",
        "    return Y_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gd_optimize(X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "\n",
        "    costs = []\n",
        "\n",
        "    X_trans = np.asarray(X).T\n",
        "    Y_trans = np.asarray(Y).T\n",
        "\n",
        "    # initialize parameters with zeros \n",
        "    w, b = initialize_with_zeros(X_trans.shape[0])\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        # Cost and gradient calculation \n",
        "        grads, cost = propagate(w,b,X_trans,Y_trans)\n",
        "        \n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        # update rule\n",
        "        w -=learning_rate*dw\n",
        "        b -=learning_rate*db\n",
        "        \n",
        "        # Record the costs\n",
        "        costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 100 training iterations\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    params = {\"w\": w,\n",
        "            \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "            \"db\": db}\n",
        "    \n",
        "    return params, grads, costs\n",
        "\n",
        "# def sgd_optimiz(X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \n",
        "#     costs = []\n",
        "\n",
        "#     X_trans = np.asarray(X).T\n",
        "#     Y_trans = np.asarray(Y).T\n",
        "\n",
        "#     # initialize parameters with zeros \n",
        "#     w, b = initialize_with_zeros(X_trans.shape[0])\n",
        "    \n",
        "#     for i in range(num_iterations):\n",
        "#         # Prepare a random subsample of data\n",
        "#         sample_len = random.randint(100,200)\n",
        "#         X_sample = X.sample(sample_len)\n",
        "#         # print(len(X_sample))\n",
        "#         Y_sample = GLOB_Y[X_sample.index]\n",
        "#         X_trans = np.asarray(X_sample).T\n",
        "#         Y_trans = np.asarray(Y_sample).T\n",
        "#         # print('Y = ', len(Y_trans))\n",
        "#         # print('X = ', len(X_trans))\n",
        "#         # Cost and gradient calculation \n",
        "#         grads, cost = propagate(w,b,X_trans,Y_trans)\n",
        "        \n",
        "#         # Retrieve derivatives from grads\n",
        "#         dw = grads[\"dw\"]\n",
        "#         db = grads[\"db\"]\n",
        "        \n",
        "#         # update rule\n",
        "#         w -=learning_rate*dw\n",
        "#         b -=learning_rate*db\n",
        "        \n",
        "#         # Record the costs\n",
        "#         costs.append(cost)\n",
        "        \n",
        "#         # Print the cost every 100 training iterations\n",
        "#         if print_cost and i % 100 == 0:\n",
        "#             print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "#     params = {\"w\": w,\n",
        "#             \"b\": b}\n",
        "    \n",
        "#     grads = {\"dw\": dw,\n",
        "#             \"db\": db}\n",
        "    \n",
        "#     return params, grads, costs\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sgd_optimize(x, y, num_iterations,learn_rate, batch_size=200, dtype=\"float64\", random_state=None, print_cost = False):\n",
        "\n",
        "    costs = []\n",
        "    dtype_ = np.dtype(dtype)\n",
        "\n",
        "    x, y = np.array(x, dtype=dtype_).T, np.array(y, dtype=dtype_).T\n",
        "    n_obs = x.shape[0]\n",
        "\n",
        "    w, b = initialize_with_zeros(n_obs)\n",
        "    x = x.T\n",
        "    bth = 200\n",
        "    num_iterations = int(num_iterations)\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        start = random.randint(1, 1400)\n",
        "        stop = random.randint(1+start, start+bth)\n",
        "        x_batch = x[start:stop]\n",
        "        x_batch = x_batch.T\n",
        "        y_batch = y[start:stop]\n",
        "\n",
        "        grads, cost = propagate(w,b,x_batch,y_batch)\n",
        "        \n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        # update rule\n",
        "        w -=learn_rate*dw\n",
        "        b -=learn_rate*db  \n",
        "        # Record the costs\n",
        "        costs.append(cost)\n",
        "            \n",
        "        # Print the cost every 100 training iterations\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    params = {\"w\": w,\n",
        "            \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "            \"db\": db}\n",
        "    \n",
        "    return params, grads, costs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdamOptim():\n",
        "    def __init__(self, zeta=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.v_dw, self.v_db = 0,0\n",
        "        self.m_dw, self.m_db = 0,0\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.eta = zeta\n",
        "    def update(self, t, w, b, dw, db):\n",
        "\n",
        "        ## momentum beta 1\n",
        "        # *** weights *** #\n",
        "        self.m_dw = self.beta1*self.m_dw + (1-self.beta1)*dw\n",
        "        # *** biases *** #\n",
        "        self.m_db = self.beta1*self.m_db + (1-self.beta1)*db\n",
        "\n",
        "        ## rms beta 2\n",
        "        # *** weights *** #\n",
        "        self.v_dw = self.beta2*self.v_dw + (1-self.beta2)*np.power( dw, 2)\n",
        "        # *** biases *** #\n",
        "        self.v_db = self.beta2*self.v_db + (1-self.beta2)*np.power( db, 2)\n",
        "\n",
        "        ## bias correction\n",
        "        m_dw_corr = self.m_dw/(1-self.beta1**t)\n",
        "        m_db_corr = self.m_db/(1-self.beta1**t)\n",
        "        v_dw_corr = self.v_dw/(1-self.beta2**t)\n",
        "        v_db_corr = self.v_db/(1-self.beta2**t)\n",
        "\n",
        "        ## update weights and biases\n",
        "        w = w - self.eta*(m_dw_corr/(np.sqrt(v_dw_corr)+self.epsilon))\n",
        "        b = b - self.eta*(m_db_corr/(np.sqrt(v_db_corr)+self.epsilon))\n",
        "        return w, b\n",
        "    def __del__(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def adam_optimize(X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \n",
        "    costs = []\n",
        "\n",
        "    X_trans = np.asarray(X).T\n",
        "    Y_trans = np.asarray(Y).T\n",
        "\n",
        "    # initialize parameters with zeros \n",
        "    w, b = initialize_with_zeros(X_trans.shape[0])\n",
        "    \n",
        "    adam = AdamOptim()\n",
        "    adam.v_dw, adam.v_db = initialize_with_zeros(X_trans.shape[0])\n",
        "    adam.m_dw, adam.m_db = initialize_with_zeros(X_trans.shape[0])\n",
        "    adam.eta = learning_rate\n",
        "    for i in range(num_iterations):\n",
        "                \n",
        "        # Cost and gradient calculation \n",
        "        grads, cost = propagate(w,b,X_trans,Y_trans)\n",
        "        \n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        w,b = adam.update(t=i+1, w=w,b=b, dw=dw, db=db)\n",
        "        \n",
        "        # Record the costs\n",
        "        costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 100 training iterations\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    del adam\n",
        "    \n",
        "    params = {\"w\": w,\n",
        "            \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "            \"db\": db}\n",
        "    \n",
        "    return params, grads, costs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Q6p8pd8pYVl1"
      },
      "outputs": [],
      "source": [
        "# predict\n",
        "\n",
        "def predict(w, b, X):\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "    \n",
        "    # Compute vector \"A\" predicting the probabilities \n",
        "    A = sigmoid(np.dot(w.T,X)+b)\n",
        "    \n",
        "    for i in range(A.shape[1]):\n",
        "        \n",
        "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
        "        if (A[0,i]<=0.5):\n",
        "            Y_prediction[0][i]=0\n",
        "        else:\n",
        "            Y_prediction[0][i]=1\n",
        "    \n",
        "    return Y_prediction\n",
        "\n",
        "# model\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, optim_fun = gd_optimize, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
        "     \n",
        "    X_train_trans = np.asarray(X_train).T\n",
        "    Y_train_trans = np.asarray(Y_train).T\n",
        "    X_test_trans = np.asarray(X_test).T\n",
        "    Y_test_trans = np.asarray(Y_test).T\n",
        "\n",
        "    # Gradient descent\n",
        "    parameters, grads, costs = optim_fun(X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "    \n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    \n",
        "    # Predict test/train set examples\n",
        "    Y_prediction_test = predict(w, b, X_test_trans)\n",
        "    Y_prediction_train = predict(w, b, X_train_trans)\n",
        "\n",
        "    # Print train/test Errors\n",
        "    traina = (100 - np.mean(np.abs(Y_prediction_train - Y_train_trans)) * 100)\n",
        "    testa = (100 - np.mean(np.abs(Y_prediction_test - Y_test_trans)) * 100)\n",
        "\n",
        "    text = f'train acc = {traina}\\ntest acc = {testa}'\n",
        "\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "\n",
        "     \n",
        "    \n",
        "    return d, text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4NWUYiqoYVl3"
      },
      "outputs": [],
      "source": [
        "# Prepare data\n",
        "df = pd.read_csv(\"bioresponse.csv\")\n",
        "GLOB_Y=df['Activity'].values\n",
        "\n",
        "# choosing numeric features\n",
        "GLOB_X = df.drop(['Activity'], axis=1, inplace=False)\n",
        "GLOB_X\n",
        "\n",
        "# Create Train and Test samples\n",
        "X_train, X_test, y_train, y_test = train_test_split(GLOB_X,GLOB_Y,test_size=0.25, random_state=68)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/1x/1zh1gh3s0gn4wj0d3z8q2x4m0000gn/T/ipykernel_12719/3759655113.py:16: RuntimeWarning: divide by zero encountered in log\n",
            "  cost = -(1./m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A),axis=1)   # compute cost\n",
            "/var/folders/1x/1zh1gh3s0gn4wj0d3z8q2x4m0000gn/T/ipykernel_12719/3759655113.py:16: RuntimeWarning: invalid value encountered in multiply\n",
            "  cost = -(1./m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A),axis=1)   # compute cost\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+-------------------------------+-------------------------------+-------------------------------+\n",
            "| Learning Rate |            GD model           |      Stochastic GD model      |           ADAM model          |\n",
            "+---------------+-------------------------------+-------------------------------+-------------------------------+\n",
            "|     0.0001    | train acc = 55.52790615001778 | train acc = 54.67472449342339 | train acc = 81.76324209029505 |\n",
            "|               | test acc = 55.223880597014926 |  test acc = 54.05117270788913 |  test acc = 76.97228144989339 |\n",
            "|     0.0005    | train acc = 68.68112335584786 | train acc = 68.57447564877356 | train acc = 87.55776750799858 |\n",
            "|               |  test acc = 69.5095948827292  |  test acc = 70.25586353944563 |  test acc = 75.69296375266525 |\n",
            "|     0.001     | train acc = 73.05367934589407 | train acc = 73.37362246711696 |  train acc = 89.4418769996445 |\n",
            "|               |  test acc = 71.74840085287846 |  test acc = 71.64179104477611 |  test acc = 75.05330490405117 |\n",
            "|     0.005     | train acc = 76.85744756487736 | train acc = 76.39530750088873 |  train acc = 93.7788837539993 |\n",
            "|               |  test acc = 76.43923240938166 |  test acc = 76.65245202558636 |  test acc = 73.02771855010661 |\n",
            "|      0.01     | train acc = 78.77710629221471 | train acc = 77.99502310700319 | train acc = 94.27657305367934 |\n",
            "|               |  test acc = 77.50533049040511 |  test acc = 76.75906183368869 |  test acc = 73.02771855010661 |\n",
            "|      0.05     | train acc = 82.18983291859226 | train acc = 80.80341272662638 | train acc = 95.59189477426236 |\n",
            "|               |  test acc = 77.9317697228145  |  test acc = 77.07889125799574 |  test acc = 71.53518123667376 |\n",
            "+---------------+-------------------------------+-------------------------------+-------------------------------+\n"
          ]
        }
      ],
      "source": [
        "lern_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]\n",
        "\n",
        "table = PrettyTable()\n",
        "gd = []\n",
        "sgdo = []\n",
        "sgd = []\n",
        "adam = []\n",
        "\n",
        "dic = {\n",
        "    'gd': [gd,gd_optimize],\n",
        "    # 'sgd_old': [sgdo,sgd_optimiz],\n",
        "    'sgd': [sgd,sgd_optimize],\n",
        "    'adam': [adam,adam_optimize]\n",
        "}\n",
        "\n",
        "table.field_names = ['Learning Rate', 'GD model', 'Stochastic GD model', 'ADAM model']\n",
        "\n",
        "for rate in lern_rates:\n",
        "    teta = []\n",
        "    teta = [str(rate)]\n",
        "    for key in dic.keys():\n",
        "        # o = dic[key][1]\n",
        "        z, rez = model(X_train, y_train, X_test, y_test, \n",
        "                            num_iterations = 2000, learning_rate = rate, \n",
        "                            print_cost = False, optim_fun = dic[key][1])\n",
        "        dic[key][0].append(z)\n",
        "        teta.append(rez)\n",
        "    table.add_row(teta)\n",
        "\n",
        "print(table)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Logistic Regression as a Neural Network - BP alg.ipynb",
      "provenance": []
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
